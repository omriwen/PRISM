{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIDS Result Analysis Tutorial\n",
    "\n",
    "Learn how to analyze and compare SPIDS experiment results.\n",
    "\n",
    "This tutorial covers:\n",
    "1. Loading saved experiments\n",
    "2. Visualizing reconstruction quality\n",
    "3. Analyzing convergence metrics\n",
    "4. Comparing multiple experiments\n",
    "\n",
    "**Estimated time**: 20-25 minutes\n",
    "\n",
    "**Prerequisites**: Run at least one SPIDS experiment first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding and Loading Experiments\n",
    "\n",
    "First, let's see what experiments are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all experiment directories\n",
    "runs_dir = Path(\"../../runs\")\n",
    "\n",
    "if runs_dir.exists():\n",
    "    experiments = sorted([d for d in runs_dir.iterdir() if d.is_dir()])\n",
    "    print(f\"Found {len(experiments)} experiments:\\n\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp.name}\")\n",
    "\n",
    "        # Check what files are available\n",
    "        files = list(exp.glob(\"*.pt\")) + list(exp.glob(\"*.json\")) + list(exp.glob(\"*.png\"))\n",
    "        if files:\n",
    "            print(f\"    Files: {len(files)} items\")\n",
    "else:\n",
    "    print(\"⚠️  No runs/ directory found. Run an experiment first!\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  cd ../..\")\n",
    "    print(\"  uv run python main.py --obj_name europa --n_samples 50 --fermat --name tutorial_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Experiment Data\n",
    "\n",
    "Load a specific experiment for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(exp_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load experiment results from the runs directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_name : str\n",
    "        Name of the experiment (folder name in runs/)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing:\n",
    "        - model_state: Final model checkpoint\n",
    "        - metrics: Training metrics (if available)\n",
    "        - config: Experiment configuration (if available)\n",
    "    \"\"\"\n",
    "    exp_dir = Path(\"../../runs\") / exp_name\n",
    "\n",
    "    if not exp_dir.exists():\n",
    "        raise ValueError(f\"Experiment '{exp_name}' not found in runs/\")\n",
    "\n",
    "    results = {\n",
    "        \"name\": exp_name,\n",
    "        \"path\": exp_dir,\n",
    "    }\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model_file = exp_dir / \"final_model.pt\"\n",
    "    if model_file.exists():\n",
    "        results[\"model_state\"] = torch.load(model_file, map_location=\"cpu\")\n",
    "        print(\"✓ Loaded model checkpoint\")\n",
    "\n",
    "    # Load metrics\n",
    "    metrics_file = exp_dir / \"metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            results[\"metrics\"] = json.load(f)\n",
    "        print(\"✓ Loaded metrics\")\n",
    "\n",
    "    # Load configuration\n",
    "    config_file = exp_dir / \"config.json\"\n",
    "    if config_file.exists():\n",
    "        with open(config_file) as f:\n",
    "            results[\"config\"] = json.load(f)\n",
    "        print(\"✓ Loaded configuration\")\n",
    "\n",
    "    # Find saved images\n",
    "    results[\"images\"] = list(exp_dir.glob(\"*.png\"))\n",
    "    print(f\"✓ Found {len(results['images'])} saved images\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Select an experiment to analyze\n",
    "# Replace with your experiment name:\n",
    "EXPERIMENT_NAME = experiments[0].name if \"experiments\" in dir() and experiments else \"tutorial_test\"\n",
    "\n",
    "print(f\"Loading experiment: {EXPERIMENT_NAME}\\n\")\n",
    "exp_data = load_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Reconstruction\n",
    "\n",
    "Display the final reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"model_state\" in exp_data:\n",
    "    # Load and visualize the reconstruction\n",
    "    from prism.models.networks import GenCropSpidsNet\n",
    "\n",
    "    # Reconstruct model (you need to know obj_size and image_size)\n",
    "    # These should ideally come from config\n",
    "    if \"config\" in exp_data:\n",
    "        obj_size = exp_data[\"config\"].get(\"obj_size\", 128)\n",
    "        image_size = exp_data[\"config\"].get(\"image_size\", 512)\n",
    "    else:\n",
    "        obj_size = 128\n",
    "        image_size = 512\n",
    "\n",
    "    model = GenCropSpidsNet(obj_size=obj_size, image_size=image_size)\n",
    "    model.load_state_dict(exp_data[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Generate reconstruction\n",
    "    with torch.no_grad():\n",
    "        reconstruction = model().cpu()\n",
    "\n",
    "    # Visualize\n",
    "    if reconstruction.shape[1] == 2:  # Complex-valued\n",
    "        magnitude = torch.sqrt(reconstruction[:, 0] ** 2 + reconstruction[:, 1] ** 2)\n",
    "        phase = torch.atan2(reconstruction[:, 1], reconstruction[:, 0])\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "        # Magnitude\n",
    "        im1 = axes[0].imshow(magnitude[0], cmap=\"viridis\")\n",
    "        axes[0].set_title(\"Magnitude\", fontsize=14, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "        plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "\n",
    "        # Phase\n",
    "        im2 = axes[1].imshow(phase[0], cmap=\"twilight\", vmin=-np.pi, vmax=np.pi)\n",
    "        axes[1].set_title(\"Phase\", fontsize=14, fontweight=\"bold\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "\n",
    "        plt.suptitle(f\"Final Reconstruction: {EXPERIMENT_NAME}\", fontsize=16, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(reconstruction[0, 0], cmap=\"viridis\")\n",
    "        plt.title(f\"Reconstruction: {EXPERIMENT_NAME}\")\n",
    "        plt.colorbar(fraction=0.046)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"⚠️  No model checkpoint found in this experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Metrics Analysis\n",
    "\n",
    "Analyze convergence and training dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"metrics\" in exp_data:\n",
    "    metrics = exp_data[\"metrics\"]\n",
    "\n",
    "    # Extract common metrics\n",
    "    losses = metrics.get(\"losses\", [])\n",
    "    coverage = metrics.get(\"coverage\", None)\n",
    "    failed_samples = metrics.get(\"failed_samples\", [])\n",
    "    n_samples = metrics.get(\"n_samples\", 0)\n",
    "\n",
    "    # Create comprehensive metrics plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # 1. Loss curve\n",
    "    if losses:\n",
    "        axes[0, 0].plot(losses, linewidth=2)\n",
    "        axes[0, 0].set_xlabel(\"Sample Number\")\n",
    "        axes[0, 0].set_ylabel(\"Loss\")\n",
    "        axes[0, 0].set_title(\"Training Loss Curve\", fontweight=\"bold\")\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_yscale(\"log\")  # Log scale often better for losses\n",
    "\n",
    "    # 2. Loss distribution\n",
    "    if losses:\n",
    "        axes[0, 1].hist(losses, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "        axes[0, 1].set_xlabel(\"Loss Value\")\n",
    "        axes[0, 1].set_ylabel(\"Frequency\")\n",
    "        axes[0, 1].set_title(\"Loss Distribution\", fontweight=\"bold\")\n",
    "        axes[0, 1].axvline(\n",
    "            np.mean(losses), color=\"r\", linestyle=\"--\", label=f\"Mean: {np.mean(losses):.4f}\"\n",
    "        )\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Success rate\n",
    "    if n_samples > 0:\n",
    "        success_rate = (n_samples - len(failed_samples)) / n_samples * 100\n",
    "        axes[1, 0].bar(\n",
    "            [\"Success\", \"Failed\"],\n",
    "            [n_samples - len(failed_samples), len(failed_samples)],\n",
    "            color=[\"green\", \"red\"],\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axes[1, 0].set_ylabel(\"Number of Samples\")\n",
    "        axes[1, 0].set_title(f\"Sample Success Rate: {success_rate:.1f}%\", fontweight=\"bold\")\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # 4. Coverage information\n",
    "    if coverage is not None:\n",
    "        axes[1, 1].text(\n",
    "            0.5,\n",
    "            0.6,\n",
    "            f\"Coverage: {coverage:.1f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=24,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        axes[1, 1].text(\n",
    "            0.5, 0.4, f\"Total Samples: {n_samples}\", ha=\"center\", va=\"center\", fontsize=16\n",
    "        )\n",
    "        if losses:\n",
    "            axes[1, 1].text(\n",
    "                0.5, 0.3, f\"Final Loss: {losses[-1]:.4f}\", ha=\"center\", va=\"center\", fontsize=16\n",
    "            )\n",
    "            axes[1, 1].text(\n",
    "                0.5, 0.2, f\"Mean Loss: {np.mean(losses):.4f}\", ha=\"center\", va=\"center\", fontsize=16\n",
    "            )\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].axis(\"off\")\n",
    "        axes[1, 1].set_title(\"Summary Statistics\", fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(f\"Metrics Analysis: {EXPERIMENT_NAME}\", fontsize=18, fontweight=\"bold\", y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    if losses:\n",
    "        print(\"Loss Statistics:\")\n",
    "        print(f\"  Mean:     {np.mean(losses):.6f}\")\n",
    "        print(f\"  Std Dev:  {np.std(losses):.6f}\")\n",
    "        print(f\"  Min:      {np.min(losses):.6f}\")\n",
    "        print(f\"  Max:      {np.max(losses):.6f}\")\n",
    "        print(f\"  Final:    {losses[-1]:.6f}\")\n",
    "    if coverage is not None:\n",
    "        print(f\"\\nCoverage: {coverage:.2f}%\")\n",
    "    if n_samples > 0:\n",
    "        print(\"\\nSamples:\")\n",
    "        print(f\"  Total:    {n_samples}\")\n",
    "        print(f\"  Success:  {n_samples - len(failed_samples)}\")\n",
    "        print(f\"  Failed:   {len(failed_samples)}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️  No metrics found in this experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Multiple Experiments\n",
    "\n",
    "Compare results from different experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_experiments(exp_names: list[str]):\n",
    "    \"\"\"\n",
    "    Compare metrics across multiple experiments.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "\n",
    "    for name in exp_names:\n",
    "        try:\n",
    "            exp = load_experiment(name)\n",
    "            if \"metrics\" in exp:\n",
    "                metrics = exp[\"metrics\"]\n",
    "                comparison_data.append(\n",
    "                    {\n",
    "                        \"name\": name,\n",
    "                        \"mean_loss\": np.mean(metrics.get(\"losses\", [])),\n",
    "                        \"coverage\": metrics.get(\"coverage\", 0),\n",
    "                        \"n_samples\": metrics.get(\"n_samples\", 0),\n",
    "                        \"failed\": len(metrics.get(\"failed_samples\", [])),\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load {name}: {e}\")\n",
    "\n",
    "    if not comparison_data:\n",
    "        print(\"No experiments to compare\")\n",
    "        return\n",
    "\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    names = [d[\"name\"] for d in comparison_data]\n",
    "\n",
    "    # Mean loss comparison\n",
    "    axes[0].bar(\n",
    "        range(len(names)),\n",
    "        [d[\"mean_loss\"] for d in comparison_data],\n",
    "        color=\"steelblue\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[0].set_xticks(range(len(names)))\n",
    "    axes[0].set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "    axes[0].set_ylabel(\"Mean Loss\")\n",
    "    axes[0].set_title(\"Mean Loss Comparison\", fontweight=\"bold\")\n",
    "    axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Coverage comparison\n",
    "    axes[1].bar(\n",
    "        range(len(names)),\n",
    "        [d[\"coverage\"] for d in comparison_data],\n",
    "        color=\"forestgreen\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[1].set_xticks(range(len(names)))\n",
    "    axes[1].set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "    axes[1].set_ylabel(\"Coverage (%)\")\n",
    "    axes[1].set_title(\"Coverage Comparison\", fontweight=\"bold\")\n",
    "    axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Success rate comparison\n",
    "    success_rates = [\n",
    "        (d[\"n_samples\"] - d[\"failed\"]) / d[\"n_samples\"] * 100 if d[\"n_samples\"] > 0 else 0\n",
    "        for d in comparison_data\n",
    "    ]\n",
    "    axes[2].bar(range(len(names)), success_rates, color=\"coral\", edgecolor=\"black\", alpha=0.7)\n",
    "    axes[2].set_xticks(range(len(names)))\n",
    "    axes[2].set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "    axes[2].set_ylabel(\"Success Rate (%)\")\n",
    "    axes[2].set_title(\"Success Rate Comparison\", fontweight=\"bold\")\n",
    "    axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "    axes[2].set_ylim(0, 100)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Experiment':<25} {'Mean Loss':>12} {'Coverage':>12} {'Success Rate':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for d in comparison_data:\n",
    "        success_rate = (\n",
    "            (d[\"n_samples\"] - d[\"failed\"]) / d[\"n_samples\"] * 100 if d[\"n_samples\"] > 0 else 0\n",
    "        )\n",
    "        print(\n",
    "            f\"{d['name']:<25} {d['mean_loss']:>12.6f} {d['coverage']:>11.1f}% {success_rate:>14.1f}%\"\n",
    "        )\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Example: Compare first 3 experiments (if available)\n",
    "if \"experiments\" in dir() and len(experiments) > 0:\n",
    "    exp_to_compare = [exp.name for exp in experiments[: min(3, len(experiments))]]\n",
    "    print(f\"Comparing {len(exp_to_compare)} experiments\\n\")\n",
    "    compare_experiments(exp_to_compare)\n",
    "else:\n",
    "    print(\"⚠️  Need at least one experiment to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quality Metrics\n",
    "\n",
    "Calculate additional quality metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(reconstruction: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate quality metrics for a reconstruction.\n",
    "    \"\"\"\n",
    "    if reconstruction.shape[1] == 2:  # Complex\n",
    "        magnitude = torch.sqrt(reconstruction[:, 0] ** 2 + reconstruction[:, 1] ** 2)\n",
    "        phase = torch.atan2(reconstruction[:, 1], reconstruction[:, 0])\n",
    "    else:\n",
    "        magnitude = reconstruction[:, 0]\n",
    "        phase = None\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"mean_intensity\": float(magnitude.mean()),\n",
    "        \"std_intensity\": float(magnitude.std()),\n",
    "        \"min_intensity\": float(magnitude.min()),\n",
    "        \"max_intensity\": float(magnitude.max()),\n",
    "        \"dynamic_range\": float(magnitude.max() - magnitude.min()),\n",
    "    }\n",
    "\n",
    "    if phase is not None:\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"phase_mean\": float(phase.mean()),\n",
    "                \"phase_std\": float(phase.std()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Calculate for loaded experiment\n",
    "if \"model_state\" in exp_data:\n",
    "    quality = calculate_quality_metrics(reconstruction)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUALITY METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in quality.items():\n",
    "        print(f\"{key:<20}: {value:>15.6f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Save analysis results for reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_report(exp_data: dict, output_file: str = \"analysis_report.json\"):\n",
    "    \"\"\"\n",
    "    Export comprehensive analysis report.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"experiment_name\": exp_data[\"name\"],\n",
    "        \"timestamp\": str(Path(exp_data[\"path\"]).stat().st_mtime),\n",
    "    }\n",
    "\n",
    "    if \"metrics\" in exp_data:\n",
    "        metrics = exp_data[\"metrics\"]\n",
    "        report[\"metrics\"] = {\n",
    "            \"mean_loss\": float(np.mean(metrics.get(\"losses\", []))),\n",
    "            \"final_loss\": float(metrics.get(\"losses\", [0])[-1]),\n",
    "            \"coverage\": metrics.get(\"coverage\"),\n",
    "            \"n_samples\": metrics.get(\"n_samples\"),\n",
    "            \"n_failed\": len(metrics.get(\"failed_samples\", [])),\n",
    "        }\n",
    "\n",
    "    # Save report\n",
    "    output_path = Path(exp_data[\"path\"]) / output_file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(f\"✓ Analysis report saved to: {output_path}\")\n",
    "    return report\n",
    "\n",
    "\n",
    "# Export current experiment analysis\n",
    "if exp_data:\n",
    "    report = export_analysis_report(exp_data)\n",
    "    print(\"\\nReport contents:\")\n",
    "    print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- ✓ How to load and explore saved experiments\n",
    "- ✓ How to visualize reconstruction results\n",
    "- ✓ How to analyze training metrics and convergence\n",
    "- ✓ How to compare multiple experiments\n",
    "- ✓ How to export analysis reports\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Run systematic experiments** - Vary parameters (n_samples, patterns, objects)\n",
    "2. **Create comparison datasets** - Build a library of experiments\n",
    "3. **Optimize parameters** - Find optimal settings for your use case\n",
    "4. **Share results** - Export reports and visualizations for papers/presentations\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "- **Python API Examples**: `examples/python_api/`\n",
    "- **Documentation**: Project README and docs/\n",
    "- **Advanced patterns**: `examples/patterns/`\n",
    "- **Baselines**: `examples/baselines/` for algorithm comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
